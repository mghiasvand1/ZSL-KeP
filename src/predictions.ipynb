{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ujson\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission, claims, retrievals = [], [], []\n",
    "\n",
    "file_path1 = '../data/test.json'\n",
    "file_path2 = '../data/retrieval.json'\n",
    "\n",
    "with open(file_path1, 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open(file_path2, 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "for item in data1:\n",
    "    dic = {\"claim_id\": item.get(\"claim_id\"), \"claim\": item.get(\"claim\"), \"pred_label\": \"\", \"evidence\": []}\n",
    "    submission.append(dic)\n",
    "    claims.append(item.get(\"claim\"))\n",
    "\n",
    "for item in data2:\n",
    "    retrievals.append(item.get(\"retrievals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\"\n",
    "\n",
    "def get_model_response(prompt):\n",
    "    system_message = {\n",
    "        'role': 'system',\n",
    "        'content': \"You are a helpful assistant with expertise in creating evidence through suitable question-answer pairs based on a given claim and the available key points within the retrieved knowledge, and in providing an accurate verdict for that claim.\"\n",
    "    }\n",
    "    user_message = {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "    }\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[system_message, user_message],\n",
    "        temperature=0,\n",
    "        top_p = 0.8,\n",
    "        max_tokens=1024,\n",
    "        seed = 42\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "folder_path = '../knowledge_store/'\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(range(len(claims))):\n",
    "    client = Groq(api_key=api_key)\n",
    "    claim = claims[i]\n",
    "    retrieval = retrievals[i]\n",
    "    prompt = \"\"\"Your task is to accurately determine a correct verdict for a given claim from the labels \"Refuted\", \"Supported\", \"Not Enough Evidence\", or \"Conflicting Evidence/Cherry-Picking\". You need to provide 1 to 4 necessary and helpful question-answer (QA) pairs. Each QA pair should be well-constructed, focusing on different important parts of the claim and utilizing the retrieved knowledge effectively to guide accurate decision-making. Therefore, you need to break down the claim into its distinct and most important subclaims, focusing on these individual components, as well as considering direct questions related to the main claim if the retrieved knowledge is sufficient. Your answers can only be in the forms of extractive (preferred), abstractive, or unanswerable. Extractive answers are those directly pulled from the text, while abstractive answers summarize or infer information based on the text. Unanswerable type is very rare, and in this case, set the answer to \"No answer could be found.\" and the citation_id to \"\". Each piece of text in the retrieved knowledge has a <citation_id> at its end, where the placeholder is replaced by the main citation ID. For each proposed answer to all answerable questions in your evidence, you must include exactly one citation ID (if there are multiple citation_id, select only one) solely within the \"citation_id\" field. After providing evidence, you must also provide a concise justification explaining how the evidence and the retrieved knowledge support the selected label for the claim. Provide your answer explicitly in the following format without any other change or additional feedback:\\n{\\n  \"evidence\": [\\n    {\\n      \"question\": \"question\",\\n      \"answer\": \"answer\",\\n      \"citation_id\": \"<citation_id>\"\\n    },\\n    ...\\n  ],\\n  \"justification\": \"justification\",\\n  \"pred_label\": \"pred_label\"\\n}\\n\\n\"\"\"+f\"\"\"Claim:\\n{claim}\\n\\nRetrieved Knowledge:\\n{retrieval}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = get_model_response(prompt)\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            response = get_model_response(prompt)\n",
    "        except Exception as e:\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                response = get_model_response(prompt)\n",
    "            except Exception as e:\n",
    "                counter += 1\n",
    "                continue\n",
    "\n",
    "    start_idx = str(response).find('{')\n",
    "    end_idx = str(response).rfind('}') + 1\n",
    "\n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        try:\n",
    "            response = ast.literal_eval(response[start_idx:end_idx])\n",
    "        except Exception as e:\n",
    "            counter += 1\n",
    "            continue\n",
    "    else:\n",
    "        counter += 1\n",
    "        continue\n",
    "\n",
    "    submission[i][\"pred_label\"] = response[\"pred_label\"]\n",
    "    for item in response[\"evidence\"]:\n",
    "        dic = {\"question\": item[\"question\"], \"answer\": item[\"answer\"], \"url\": \"\", \"scraped_text\": \"\"}\n",
    "        if item[\"citation_id\"] != \"\":\n",
    "            if \"<\" in item[\"citation_id\"]:\n",
    "                item[\"citation_id\"] = item[\"citation_id\"].replace(\"<\", \"\")\n",
    "                item[\"citation_id\"] = item[\"citation_id\"].replace(\">\", \"\")\n",
    "            if \",\" in item[\"citation_id\"]:\n",
    "                item[\"citation_id\"] = item[\"citation_id\"].split(\",\")[0].strip()\n",
    "            url_index = int(item[\"citation_id\"].split(\"_\")[0])\n",
    "            scraped_text_index = int(item[\"citation_id\"].split(\"_\")[1])\n",
    "            file_path = folder_path + f\"{i}.json\"\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for current_index, line in enumerate(file):\n",
    "                    if current_index == url_index:\n",
    "                        row = ujson.loads(line)\n",
    "                        break\n",
    "            dic[\"url\"] = row[\"url\"]\n",
    "            dic[\"scraped_text\"] = row[\"url2text\"][scraped_text_index]\n",
    "        submission[i][\"evidence\"].append(dic)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "with open('../data/predictions.json', 'w') as f:\n",
    "    json.dump(submission, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
